{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR Model Training and Prediction Pipeline\n",
    "\n",
    "## ⚠️ URGENT FIX FOR \"ValueError: numpy.dtype size changed\" ⚠️\n",
    "\n",
    "You are seeing this error because **Numpy 2.0** is installed, but **TensorFlow** requires **Numpy 1.x**.\n",
    "\n",
    "**INSTRUCTIONS:**\n",
    "1.  **Run the cell below** (it uses `%pip` to fix your specific kernel).\n",
    "2.  **Wait** for it to finish uninstalling and installing.\n",
    "3.  **Click 'Kernel' -> 'Restart Kernel'** in the top menu.\n",
    "4.  **Run the verification cell** (Cell 2) to confirm Numpy is version 1.2x.x.\n",
    "5.  Then run the rest of the notebook.\n",
    "\n",
    "*Note: If you get 'Permission denied' errors, you verify you are running Jupyter as Administrator, or run `pip install \"numpy<2\"` in your Anaconda Prompt.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 1.26.4\n",
      "Uninstalling numpy-1.26.4:\n",
      "  Successfully uninstalled numpy-1.26.4\n",
      "Found existing installation: h5py 3.15.1\n",
      "Uninstalling h5py-3.15.1:\n",
      "  Successfully uninstalled h5py-3.15.1\n",
      "Found existing installation: tensorflow 2.20.0\n",
      "Uninstalling tensorflow-2.20.0:\n",
      "  Successfully uninstalled tensorflow-2.20.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting numpy<2.0\n",
      "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.20.0-cp312-cp312-win_amd64.whl.metadata (4.6 kB)\n",
      "Collecting h5py\n",
      "  Using cached h5py-3.15.1-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: emnist in c:\\programdata\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-image in c:\\programdata\\anaconda3\\lib\\site-packages (0.23.2)\n",
      "Requirement already satisfied: opencv-python in c:\\programdata\\anaconda3\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (6.33.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.12.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from emnist) (4.66.4)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image) (1.16.3)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image) (3.2.1)\n",
      "Requirement already satisfied: pillow>=9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image) (10.3.0)\n",
      "Requirement already satisfied: imageio>=2.33 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image) (2023.4.12)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image) (0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\talha\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->emnist) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\talha\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Using cached tensorflow-2.20.0-cp312-cp312-win_amd64.whl (331.9 MB)\n",
      "Using cached h5py-3.15.1-cp312-cp312-win_amd64.whl (2.9 MB)\n",
      "Installing collected packages: numpy, h5py, tensorflow\n",
      "Successfully installed h5py-3.15.1 numpy-1.26.4 tensorflow-2.20.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.32.0 requires protobuf<5,>=3.20, but you have protobuf 6.33.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# CRITICAL FIX: Use %pip magic to ensure we install into the CURRENT kernel\n",
    "# We force uninstall numpy and reinstall a compatible version (<2.0)\n",
    "%pip uninstall -y numpy h5py tensorflow\n",
    "%pip install \"numpy<2.0\" tensorflow h5py emnist scikit-image opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Numpy Version: 1.26.4\n",
      "SUCCESS: Numpy version is compatible. Proceeding to import TensorFlow.\n"
     ]
    }
   ],
   "source": [
    "# VERIFICATION CELL\n",
    "# Run this AFTER restarting the kernel. \n",
    "# It must print a version starting with '1.' (e.g., 1.26.4). \n",
    "# If it says '2.0.0', the fix didn't work and you need to run the pip command in your terminal.\n",
    "import numpy as np\n",
    "print(f\"Current Numpy Version: {np.__version__}\")\n",
    "\n",
    "if np.__version__.startswith(\"2\"):\n",
    "    raise RuntimeError(\"STOP! Numpy 2.0 is still active. Please restart the kernel again. If that fails, run 'pip install \\\"numpy<2\\\"' in your command prompt.\")\n",
    "else:\n",
    "    print(\"SUCCESS: Numpy version is compatible. Proceeding to import TensorFlow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from skimage.measure import label, regionprops\n",
    "from emnist import extract_training_samples, extract_test_samples\n",
    "\n",
    "# Check for GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare EMNIST Data\n",
    "We use the **Balanced** split (47 classes: 0-9, A-Z, a-z mapping merged for similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EMNIST data... (This might take a moment to download)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading emnist.zip: 32.0kB [00:00, 5.97MB/s]\n"
     ]
    },
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadZipFile\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading EMNIST data... (This might take a moment to download)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m X_train, y_train = \u001b[43mextract_training_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbalanced\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m X_test, y_test = extract_test_samples(\u001b[33m'\u001b[39m\u001b[33mbalanced\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# IMPORTANT: EMNIST images are rotated 90 degrees and flipped by default.\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# We need to transpose them to look like normal characters.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\emnist\\__init__.py:209\u001b[39m, in \u001b[36mextract_training_samples\u001b[39m\u001b[34m(dataset)\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_training_samples\u001b[39m(dataset):\n\u001b[32m    207\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Extract the training samples for a given dataset as a pair of numpy arrays, (images, labels). The dataset must be\u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[33;03m    one of those listed by list_datasets(), e.g. 'digits' or 'mnist'.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mextract_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\emnist\\__init__.py:199\u001b[39m, in \u001b[36mextract_samples\u001b[39m\u001b[34m(dataset, usage)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_samples\u001b[39m(dataset, usage):\n\u001b[32m    197\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Extract the samples for a given dataset and usage as a pair of numpy arrays, (images, labels). The dataset must\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[33;03m    be one of those listed by list_datasets(), e.g. 'digits' or 'mnist'. Usage should be either 'train' or 'test'.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     images = \u001b[43mextract_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimages\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m     labels = extract_data(dataset, usage, \u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(images) != \u001b[38;5;28mlen\u001b[39m(labels):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\emnist\\__init__.py:185\u001b[39m, in \u001b[36mextract_data\u001b[39m\u001b[34m(dataset, usage, component)\u001b[39m\n\u001b[32m    183\u001b[39m cache_path = get_cached_data_path()\n\u001b[32m    184\u001b[39m zip_internal_path = ZIP_PATH_TEMPLATE.format(dataset=dataset, usage=usage, matrix=component, dim=dim)\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m zf:\n\u001b[32m    186\u001b[39m     compressed_data = zf.read(zip_internal_path)\n\u001b[32m    187\u001b[39m data = gzip.decompress(compressed_data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\zipfile\\__init__.py:1349\u001b[39m, in \u001b[36mZipFile.__init__\u001b[39m\u001b[34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[39m\n\u001b[32m   1347\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1348\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1349\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_RealGetContents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1350\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mx\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m   1351\u001b[39m         \u001b[38;5;66;03m# set the modified flag so central directory gets written\u001b[39;00m\n\u001b[32m   1352\u001b[39m         \u001b[38;5;66;03m# even if no files are added to the archive\u001b[39;00m\n\u001b[32m   1353\u001b[39m         \u001b[38;5;28mself\u001b[39m._didModify = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\Lib\\zipfile\\__init__.py:1416\u001b[39m, in \u001b[36mZipFile._RealGetContents\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1414\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[33m\"\u001b[39m\u001b[33mFile is not a zip file\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1415\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m endrec:\n\u001b[32m-> \u001b[39m\u001b[32m1416\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BadZipFile(\u001b[33m\"\u001b[39m\u001b[33mFile is not a zip file\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1417\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.debug > \u001b[32m1\u001b[39m:\n\u001b[32m   1418\u001b[39m     \u001b[38;5;28mprint\u001b[39m(endrec)\n",
      "\u001b[31mBadZipFile\u001b[39m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "print(\"Loading EMNIST data... (This might take a moment to download)\")\n",
    "X_train, y_train = extract_training_samples('balanced')\n",
    "X_test, y_test = extract_test_samples('balanced')\n",
    "\n",
    "# IMPORTANT: EMNIST images are rotated 90 degrees and flipped by default.\n",
    "# We need to transpose them to look like normal characters.\n",
    "X_train = np.array([image.T for image in X_train])\n",
    "X_test = np.array([image.T for image in X_test])\n",
    "\n",
    "# Normalize to [0, 1]\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape for CNN (28, 28, 1)\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "X_test = np.expand_dims(X_test, -1)\n",
    "\n",
    "# One-hot encode labels\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train_cat = to_categorical(y_train, num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "print(f\"Data Loaded. Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Mapping for EMNIST Balanced\n",
    "label_map = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabdefghnqrt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train the CNN Model\n",
    "We will train a Convolutional Neural Network and save it as `ocr_model.h5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Starting training...\")\n",
    "history = model.fit(X_train, y_train_cat, epochs=10, batch_size=128, validation_data=(X_test, y_test_cat), verbose=1)\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_filename = 'ocr_model.h5'\n",
    "model.save(model_filename)\n",
    "print(f\"Model saved as {model_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predict on Handwriting Images\n",
    "Now we use the trained model to find and recognize text in your images.\n",
    "\n",
    "**Optimizations inserted:**\n",
    "- **Noise Removal:** Median Blur + Morphological Opening.\n",
    "- **Sorting:** Characters are sorted top-to-bottom, then left-to-right to handle multiple lines correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_regions_reading_order(regions, line_threshold=15):\n",
    "    \"\"\"\n",
    "    Sorts regions in reading order (Top -> Bottom, Left -> Right).\n",
    "    'line_threshold' is the pixel tolerance to consider regions on the same line.\n",
    "    \"\"\"\n",
    "    # 1. Sort all by Y-coordinate (top to bottom)\n",
    "    regions = sorted(regions, key=lambda r: r.bbox[0])\n",
    "    \n",
    "    lines = []\n",
    "    current_line = []\n",
    "    current_y = regions[0].bbox[0]\n",
    "    \n",
    "    for r in regions:\n",
    "        # If this region is significantly lower, start a new line\n",
    "        if r.bbox[0] > current_y + line_threshold:\n",
    "            # Sort the completed line by X-coordinate (left to right)\n",
    "            current_line.sort(key=lambda r: r.bbox[1])\n",
    "            lines.extend(current_line)\n",
    "            \n",
    "            # Start new line\n",
    "            current_line = [r]\n",
    "            current_y = r.bbox[0]\n",
    "        else:\n",
    "            current_line.append(r)\n",
    "            \n",
    "    # Append the last line\n",
    "    current_line.sort(key=lambda r: r.bbox[1])\n",
    "    lines.extend(current_line)\n",
    "    \n",
    "    return lines\n",
    "\n",
    "def prepare_segment(segment, target_size=28):\n",
    "    \"\"\"\n",
    "    Resizes and pads a character segment to fit the 28x28 input of the CNN.\n",
    "    Keeps aspect ratio and centers the image.\n",
    "    \"\"\"\n",
    "    h, w = segment.shape\n",
    "    if h == 0 or w == 0: return None\n",
    "    \n",
    "    # Add padding to aspect ratio\n",
    "    padding = 4\n",
    "    scale = (target_size - 2*padding) / max(h, w)\n",
    "    new_h, new_w = int(h * scale), int(w * scale)\n",
    "    \n",
    "    resized = cv2.resize(segment, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "    canvas = np.zeros((target_size, target_size), dtype='uint8')\n",
    "    \n",
    "    # Calculate center offset\n",
    "    y_off = (target_size - new_h) // 2\n",
    "    x_off = (target_size - new_w) // 2\n",
    "    \n",
    "    canvas[y_off:y_off+new_h, x_off:x_off+new_w] = resized\n",
    "    return canvas\n",
    "\n",
    "def predict_receipt(img_path, model):\n",
    "    if not os.path.exists(img_path):\n",
    "        print(f\"File not found: {img_path}\")\n",
    "        return\n",
    "\n",
    "    # 1. Read Image\n",
    "    original = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(original, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # 2. Preprocess (Noise Cleaning)\n",
    "    # Gaussian Blur to smooth edges\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    # Adaptive Threshold to binarize\n",
    "    binary = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "    # Morphological clean up\n",
    "    kernel = np.ones((2,2), np.uint8)\n",
    "    opened = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    # 3. Find Char Candidates\n",
    "    lbl = label(opened)\n",
    "    regions = regionprops(lbl)\n",
    "    \n",
    "    # Filter noise by size\n",
    "    valid_regions = []\n",
    "    img_area = gray.shape[0] * gray.shape[1]\n",
    "    for r in regions:\n",
    "        h = r.bbox[2] - r.bbox[0]\n",
    "        w = r.bbox[3] - r.bbox[1]\n",
    "        if h > 10 and w > 5 and h*w < img_area * 0.1: # Min size 10x5, Max 10% of image\n",
    "            valid_regions.append(r)\n",
    "            \n",
    "    if not valid_regions:\n",
    "        print(f\"No text found in {os.path.basename(img_path)}\")\n",
    "        return\n",
    "\n",
    "    # 4. Sort Candidates (Reading Order)\n",
    "    sorted_regions = sort_regions_reading_order(valid_regions)\n",
    "    \n",
    "    # 5. Predict\n",
    "    predicted_text = \"\"\n",
    "    annotated_img = original.copy()\n",
    "    \n",
    "    # To detect spaces, we check horizontal distance between characters\n",
    "    last_max_col = 0\n",
    "    \n",
    "    for i, r in enumerate(sorted_regions):\n",
    "        minr, minc, maxr, maxc = r.bbox\n",
    "        \n",
    "        # Check for space (heuristic: if distance > 15 pixels, add space)\n",
    "        # Only if we are on the same line (this logic is simplified, works for single lines well)\n",
    "        if i > 0 and (minc - last_max_col) > 20:\n",
    "            predicted_text += \" \"\n",
    "        last_max_col = maxc\n",
    "\n",
    "        segment = opened[minr:maxr, minc:maxc]\n",
    "        nn_input = prepare_segment(segment)\n",
    "        \n",
    "        if nn_input is not None:\n",
    "            # Normalize and Reshape\n",
    "            nn_input = nn_input.astype('float32') / 255.0\n",
    "            nn_input = np.expand_dims(nn_input, -1)\n",
    "            nn_input = np.expand_dims(nn_input, 0)\n",
    "            \n",
    "            prediction = model.predict(nn_input, verbose=0)\n",
    "            char_idx = np.argmax(prediction)\n",
    "            char = label_map[char_idx]\n",
    "            predicted_text += char\n",
    "            \n",
    "            # Visuals\n",
    "            cv2.rectangle(annotated_img, (minc, minr), (maxc, maxr), (0, 255, 0), 2)\n",
    "            cv2.putText(annotated_img, char, (minc, minr-5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "    # Show Result\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f\"Prediction: {predicted_text}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    print(f\"File: {os.path.basename(img_path)} | Text: {predicted_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run prediction on all images in folder\n",
    "input_folder = \"handwritten-receipts\"\n",
    "images = glob.glob(os.path.join(input_folder, \"*.jpg\")) + glob.glob(os.path.join(input_folder, \"*.png\"))\n",
    "\n",
    "print(f\"Found {len(images)} images to process.\")\n",
    "for img in images:\n",
    "    with tf.device('/CPU:0'): # Force CPU if CUDA OOM issues occur with small models\n",
    "        predict_receipt(img, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
